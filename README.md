<h1 align="center" id="title">Application of Different Activation Functions on Dataset</h1>

<p id="description">The following project will examine the efficiency of 3 different activation functions ( Sigmoid | Tahn | Relu ) on MNIST dataset.</p>

  
  
<h2>🧐 Features</h2>

Here're some of the project's best features:

*   The project runs on Cuda GPU
*   Usage of graphs for deeper understanding

<h2>🛠️ Installation Steps:</h2>

<p>1. Download spec-file.txt</p>

<p>2. Follow the instructions into Conda_Virtual_Env.txt</p>

  
  
<h2>💻 Built with</h2>

Technologies used in the project:

*   Python
*   Pytorch
*   Matplotlib 
